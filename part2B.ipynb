{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the same as part 2, but a conditional GAN instead of plain DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets,layers,models\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00046 data_20176\n",
      "00048 data_20176\n",
      "00047 data_20176\n",
      "00049 data_20176\n",
      "00045 data_20176\n",
      "1928\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "data_dirs = [d for d in os.listdir('dreamhouse_data/')]\n",
    "\n",
    "paths = []\n",
    "costs = []\n",
    "years = []\n",
    "beds = []\n",
    "baths = []\n",
    "for dd in data_dirs:\n",
    "    houses_found = [d for d in os.listdir('dreamhouse_data/'+dd) if os.path.isdir('dreamhouse_data/{}/{}'.format(dd,d))]\n",
    "    with open('dreamhouse_data/{}/metadata.json'.format(dd)) as test_md:\n",
    "        md = json.load(test_md)\n",
    "    for house in houses_found:\n",
    "        house_content = os.listdir('dreamhouse_data/{}/{}'.format(dd,house))\n",
    "        if 'gsv_0.jpg' in house_content:\n",
    "            try:\n",
    "                costs.append(md[house]['PRICE'])\n",
    "                years.append(md[house]['YEAR BUILT'])\n",
    "                beds.append(md[house]['BEDS'])\n",
    "                baths.append(md[house]['BATHS'])\n",
    "                paths.append('dreamhouse_data/{}/{}/gsv_0.jpg'.format(dd,house))\n",
    "            except KeyError:\n",
    "                print(house,dd)\n",
    "print(len(paths))\n",
    "print(len(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's turn the price of the home into categorical labels (i.e. cost bins by 400000s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>costs</th>\n",
       "      <th>paths</th>\n",
       "      <th>bin</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1095000</td>\n",
       "      <td>dreamhouse_data/data_22207/00028/gsv_0.jpg</td>\n",
       "      <td>(1000000.0, 1400000.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2200000</td>\n",
       "      <td>dreamhouse_data/data_22207/00011/gsv_0.jpg</td>\n",
       "      <td>(1800000.0, 2200000.0]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>824888</td>\n",
       "      <td>dreamhouse_data/data_22207/00037/gsv_0.jpg</td>\n",
       "      <td>(600000.0, 1000000.0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1700000</td>\n",
       "      <td>dreamhouse_data/data_22207/00003/gsv_0.jpg</td>\n",
       "      <td>(1400000.0, 1800000.0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1150000</td>\n",
       "      <td>dreamhouse_data/data_22207/00029/gsv_0.jpg</td>\n",
       "      <td>(1000000.0, 1400000.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     costs                                       paths  \\\n",
       "0  1095000  dreamhouse_data/data_22207/00028/gsv_0.jpg   \n",
       "1  2200000  dreamhouse_data/data_22207/00011/gsv_0.jpg   \n",
       "2   824888  dreamhouse_data/data_22207/00037/gsv_0.jpg   \n",
       "3  1700000  dreamhouse_data/data_22207/00003/gsv_0.jpg   \n",
       "4  1150000  dreamhouse_data/data_22207/00029/gsv_0.jpg   \n",
       "\n",
       "                      bin cat  \n",
       "0  (1000000.0, 1400000.0]   2  \n",
       "1  (1800000.0, 2200000.0]   4  \n",
       "2   (600000.0, 1000000.0]   1  \n",
       "3  (1400000.0, 1800000.0]   3  \n",
       "4  (1000000.0, 1400000.0]   2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df = pd.DataFrame({'costs':costs,'paths':paths})\n",
    "bins = np.array([0,600000,1000000,1400000,1800000,2200000,float(np.Inf)])\n",
    "price_df['bin'] = pd.cut(price_df.costs,bins)\n",
    "cuts = list(set(price_df['bin'].to_list()))\n",
    "cuts = sorted(cuts)\n",
    "bin_ids = {cuts[i]:i for i in range(len(cuts))}\n",
    "price_df['cat'] = price_df['bin'].map(bin_ids)\n",
    "print(set(price_df['cat'].to_list()))\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([276, 560, 272, 280, 206, 334]),\n",
       " array([      0.,  600000., 1000000., 1400000., 1800000., 2200000.,\n",
       "             inf]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(costs,bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(path):\n",
    "    im = tf.io.read_file(path)\n",
    "    im = tf.image.decode_jpeg(im,channels=3)\n",
    "    im = tf.image.resize(im,[IM_SIZE,IM_SIZE])\n",
    "    #im /= 255.0\n",
    "    im = tf.cast(im,tf.float32)\n",
    "    im = (im/127.5)-1\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_label(cat):\n",
    "    return np.array(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = len(list(set(price_df['cat'].to_list())))\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "image_ds = path_ds.map(load_and_preprocess,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(price_df['cat'].to_list())\n",
    "#label_ds = int_ds.map(proc_label,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "image_label_ds = tf.data.Dataset.zip((image_ds,label_ds)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. mergining the categorical label and the image using layers.Concatenate() was from https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(latent_dim, n_classes=6):\n",
    "    \n",
    "    in_label = layers.Input(shape=(1,))\n",
    "    li = layers.Embedding(n_classes, 50)(in_label)\n",
    "    \n",
    "    li = layers.Dense(7*7, use_bias=False, input_shape=(100,))(li)\n",
    "    li = layers.Reshape((7,7,1))(li)\n",
    "    \n",
    "    in_lat = layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    gen = layers.Dense(7*7*256)(in_lat)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU()(gen)\n",
    "    gen = layers.Reshape((7,7,256))(gen)\n",
    "    \n",
    "    merge = layers.Concatenate()([gen,li])\n",
    "    \n",
    "    gen = layers.Conv2DTranspose(128, (5,5), strides = (1,1), padding='same', use_bias=False)(merge)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU()(gen)\n",
    "    \n",
    "    gen = layers.Conv2DTranspose(64, (5,5), strides = (2,2), padding='same', use_bias=False)(gen)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU()(gen)\n",
    "    \n",
    "    gen = layers.Conv2DTranspose(32, (5,5), strides = (2,2), padding='same', use_bias=False)(gen)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU()(gen)\n",
    "    \n",
    "    gen = layers.Conv2DTranspose(16, (5,5), strides = (2,2), padding='same', use_bias=False)(gen)\n",
    "    gen = layers.BatchNormalization()(gen)\n",
    "    gen = layers.LeakyReLU()(gen)\n",
    "    \n",
    "    gen = layers.Conv2DTranspose(8, (5,5), strides = (2,2), padding='same', use_bias=False)(gen)\n",
    "    \n",
    "    out_layer = layers.Conv2DTranspose(3, (5,5), strides = (2,2), padding='same', use_bias=False, activation='tanh')(gen)\n",
    "   \n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12544)        1266944     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 12544)        50176       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 50)        300         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 12544)        0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 49)        2450        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 7, 7, 256)    0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 7, 7, 1)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7, 7, 257)    0           reshape_1[0][0]                  \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 7, 7, 128)    822400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 7, 128)    512         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 64)   204800      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 14, 14, 64)   256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 14, 14, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 32)   51200       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 32)   128         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 28, 28, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 56, 56, 16)   12800       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 16)   64          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 112, 112, 8)  3200        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 224, 224, 3)  600         conv2d_transpose_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,415,830\n",
      "Trainable params: 2,390,262\n",
      "Non-trainable params: 25,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model(100)\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples, n_classes=6):\n",
    "    x_input = np.random.randn(latent_dim*n_samples)\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    labels = np.random.randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_noise,gen_lab = generate_latent_points(100,5)\n",
    "generated_image = generator([gen_noise,gen_lab]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42c6eed5f8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(generated_image.shape)\n",
    "plt.imshow(generated_image[0,:,:,:]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(in_shape=(224,224,3),n_classes=6):\n",
    "    \n",
    "    in_label = layers.Input(shape=(1,))\n",
    "    \n",
    "    li = layers.Embedding(n_classes, 50)(in_label)\n",
    "    \n",
    "    n_nodes = in_shape[0]*in_shape[1]\n",
    "    li = layers.Dense(n_nodes)(li)\n",
    "    li = layers.Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    \n",
    "    in_image = layers.Input(shape=in_shape)\n",
    "    \n",
    "    merge = layers.Concatenate()([in_image, li])\n",
    "    \n",
    "    fe = layers.Conv2D(64, (5,5), strides=(2,2), padding='same')(merge)\n",
    "    fe = layers.LeakyReLU()(fe)\n",
    "    fe = layers.Dropout(0.3)(fe)\n",
    "    \n",
    "    fe = layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
    "    fe = layers.LeakyReLU()(fe)\n",
    "    fe = layers.Dropout(0.3)(fe)\n",
    "    \n",
    "    fe = layers.Flatten()(fe)\n",
    "    \n",
    "    out_layer = layers.Dense(1,activation='sigmoid')(fe)\n",
    "    \n",
    "    model = Model([in_image, in_label],out_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        300         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 50176)     2558976     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 224, 224, 1)  0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 224, 224, 4)  0           input_4[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 6464        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 112, 112, 64) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 112, 112, 64) 0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 128)  204928      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 56, 56, 128)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 56, 56, 128)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 401408)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            401409      flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,172,077\n",
      "Trainable params: 3,172,077\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fake_thing = generate_fake_data_w_labels(generated_image,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.49994573]\n",
      " [0.4998594 ]\n",
      " [0.5000449 ]\n",
      " [0.49989226]\n",
      " [0.49997863]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decision = discriminator([generated_image,tf.zeros(5)])\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5146471 ]\n",
      " [0.4917453 ]\n",
      " [0.4946757 ]\n",
      " [0.50606   ]\n",
      " [0.46955943]\n",
      " [0.50879186]\n",
      " [0.48385784]\n",
      " [0.49734426]\n",
      " [0.5020306 ]\n",
      " [0.49496728]\n",
      " [0.47830045]\n",
      " [0.50244135]\n",
      " [0.4747903 ]\n",
      " [0.48549378]\n",
      " [0.47640073]\n",
      " [0.50107855]\n",
      " [0.50164336]\n",
      " [0.47893035]\n",
      " [0.490783  ]\n",
      " [0.47932655]\n",
      " [0.4910807 ]\n",
      " [0.5169817 ]\n",
      " [0.50533646]\n",
      " [0.47356516]\n",
      " [0.516596  ]\n",
      " [0.47503388]\n",
      " [0.48301825]\n",
      " [0.46884325]\n",
      " [0.5259909 ]\n",
      " [0.50782627]\n",
      " [0.50244135]\n",
      " [0.5176044 ]\n",
      " [0.47192493]\n",
      " [0.5042161 ]\n",
      " [0.4943225 ]\n",
      " [0.48901248]\n",
      " [0.4803097 ]\n",
      " [0.5032855 ]\n",
      " [0.50810117]\n",
      " [0.49496728]\n",
      " [0.4885654 ]\n",
      " [0.4943225 ]\n",
      " [0.51434654]\n",
      " [0.4907122 ]\n",
      " [0.4963695 ]\n",
      " [0.4987569 ]\n",
      " [0.49998274]\n",
      " [0.49924216]\n",
      " [0.50080454]\n",
      " [0.4828935 ]\n",
      " [0.5071763 ]\n",
      " [0.4898092 ]\n",
      " [0.50813   ]\n",
      " [0.5043255 ]\n",
      " [0.51219666]\n",
      " [0.48723117]\n",
      " [0.49186885]\n",
      " [0.5176044 ]\n",
      " [0.48067904]\n",
      " [0.49839193]\n",
      " [0.48332968]\n",
      " [0.48196638]\n",
      " [0.49653614]\n",
      " [0.49279484]], shape=(64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for dat in image_label_ds.take(1):\n",
    "    print(discriminator(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output,fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cgan_ckpts’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir cgan_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'cgan_ckpts'\n",
    "checkpoint_pref = os.path.join(checkpoint_dir,'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,generator=generator,discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "noise_dim=100\n",
    "num_examples_to_generate=9\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(data):\n",
    "    \n",
    "    tr = generate_latent_points(100,BATCH_SIZE)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(tr)\n",
    "                 \n",
    "        real_output = discriminator(data)        \n",
    "        fake_output = discriminator([generated_images,tf.zeros(BATCH_SIZE)])\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output,fake_output)\n",
    "        \n",
    "    grad_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    grad_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(grad_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for data_batch in dataset:\n",
    "            train_step(data_batch)\n",
    "            \n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch+1, seed)\n",
    "        \n",
    "        if (epoch+1)%5 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_pref)\n",
    "            \n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,epochs,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cgan_frames’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir cgan_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model,epoch,test_input):\n",
    "    pred = model(test_input,training=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    \n",
    "    for i in range(pred.shape[0]):\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(pred[i,:,:,:]*0.5+0.5)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.savefig('cgan_frames/im_at_epoch{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-672b2b8bae67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_label_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-9189c81261df>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-bcc280b980c8>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mgrad_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mgrad_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1253\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(image_label_ds,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_file = f'gan_{EPOCHS}.gif'\n",
    "\n",
    "with imageio.get_writer(g_file,mode='I') as writer:\n",
    "    fnames = glob.glob('gan_frames/*.png')\n",
    "    fnames = sorted(fnames)\n",
    "    for f in fnames:\n",
    "        im = imageio.imread(f)\n",
    "        writer.append_data(im)\n",
    "    image = imageio.imread(f)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(g_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
